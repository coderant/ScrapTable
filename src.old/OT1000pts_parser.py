#!/usr/bin/pythonimport getoptimport randomimport reimport sysimport timefrom bs4 import BeautifulSoup as bsfrom selenium import webdriverout_delimiter = ';'mindelay = 10maxdelay = 12ackdelay = 2random.seed()whitespace = re.compile(r'\s+')def delay():    currentdelay = random.randint(mindelay, maxdelay)    print "sleeping for {0} seconds...".format(currentdelay)    time.sleep(currentdelay)def wait(length=ackdelay):    time.sleep(length)def acknowledge(browser):    time.sleep(ackdelay)  # form needs time to stabilize; 1 sec enough, 2 for good measure    try:        alert = browser.switch_to_alert()        alert.dismiss()    except:        passfield_index = {i: n for i, n in               enumerate([                   "name", "date", "time"])               }def parser1(date, time, soup, outfile):    rows = soup.find("table", {'id': "gridPromoGrid"}).findAll("td", {'class', "restNameColumn"})    restaurants = {}    j = 0    for element in rows:        j = j + 1        restaurants['name'] = element.a.get_text()        restaurants['time'] = time        restaurants['date'] = date        outfile.write(out_delimiter.join(            [re.sub(whitespace, ' ', unicode(restaurants[field_index[i]]).encode('utf-8', 'ignore')) for i in             range(len(field_index))]        ))        outfile.write('\n')    print "Done processing {0} records.".format(j)def parser2(date, time, soup, outfile):    rows = soup.find("table", {'id': "SearchResults_ResultsGrid"}).findAll("td", {'class', "ReCol"})    restaurants = {}    j = 0    for element in rows:        j = j + 1        restaurants['name'] = element.a.get_text()        restaurants['time'] = time        restaurants['date'] = date        outfile.write(out_delimiter.join(            [re.sub(whitespace, ' ', unicode(restaurants[field_index[i]]).encode('utf-8', 'ignore')) for i in             range(len(field_index))]        ))        outfile.write('\n')    print "Done processing {0} records.".format(j)def process(outputfilename):    with open(outputfilename, 'a', 1) as outfile:        browser = webdriver.Chrome()        url = raw_input("Enter URL to start crawling. In not entered, will start on 2009-11-29:")        if (url == ""):            url = "http://web.archive.org/web/20091129235006/http://www.opentable.com/diprogram.aspx?m=8&ref=1203"        browser.get(url)        matchObj = re.match(r'(.*\/(\d{8})(\d{6}).*)', browser.current_url, re.I)        date = matchObj.group(2)        time = matchObj.group(3)        print("Processing capture on {0} at {1} hours".format(date, time))        soup = bs((browser.page_source).encode('ascii', 'ignore'), "html.parser")        if soup.find("table", {'id': "gridPromoGrid"}) != None:            parser1(date, time, soup, outfile)        elif soup.find("table", {'id': "SearchResults_ResultsGrid"}) != None:            parser2(date, time, soup, outfile)        else:            print "I don't know how to parse this page"        cont = raw_input("Continue with next page (y/n)?")        while (cont == 'y'):            browser.find_element_by_xpath(                '/html/body/div[1]/div/div/table/tbody/tr/td[2]/table/tbody/tr[1]/td[2]/table/tbody/tr[2]/td[3]/a/img').click()            matchObj = re.match(r'(.*\/(\d{8})(\d{6}).*)', browser.current_url, re.I)            date = matchObj.group(2)            time = matchObj.group(3)            print("Processing capture on {0} at {1} hours".format(date, time))            soup = bs((browser.page_source).encode('ascii', 'ignore'), "html.parser")            if soup.find("table", {'id': "gridPromoGrid"}) != None:                parser1(date, time, soup, outfile)            elif soup.find("table", {'id': "SearchResults_ResultsGrid"}) != None:                parser2(date, time, soup, outfile)            else:                print "I don't know how to parse this page"                # cont = raw_input("Continue with next page (y/n)?")            delay()            cont = "y"        browser.quit()usage = "OT1000pts_parser.py -o <outfile>"def main(argv):    outputfilename = None    try:        opts, args = getopt.getopt(sys.argv[1:], "ho", ["help", "ofile="])        for opt, arg in opts:            if opt == '-h':                print usage                sys.exit()            elif opt in ("-o", "--ofile"):                outputfilename = arg    except getopt.GetoptError:        print usage        sys.exit(2)    except:        print usage        sys.exit(3)    if outputfilename == None:        print "ERROR: no input file provided, use OTparser.py -h for help"        sys.exit(4)    if outputfilename[-4:] != '.csv':        print outputfilename        print "ERROR: expect output file to be of type csv"        sys.exit(5)    try:        with open(outputfilename):            print "output file '{0}' already exists, appending new results".format(outputfilename)    except IOError:        with open(outputfilename, 'w') as outfile:            outfile.write(out_delimiter.join([field_index[i] for i in range(len(field_index))]))            outfile.write("\n")    process(outputfilename)if __name__ == "__main__":    main(sys.argv[1:])